\graphicspath{{chapters/06_discussion/}}
\chapter{Discussion}

% The Discussion section contains a critical analysis of the results obtained and frames them in the context of the international literature (at least 5, maximum 10 pages).

% PANCALDI SOMEWHERE?

%%%%%%%%%%%%% ABOUT THE ANALYSIS %%%%%%%%%%%%%%%%%%%%%%%

\section{Pixel preprocessing}

One concern that might be raised in regards to the preprocessing pipeline adopted by HiCONA is the lack of a sequencing bias normalization step. There are two main reasons for that. First of all, sequencing biases are expected to affect the pixel counts way less than genomic distance, to the point where it is not unreasonable to assume that the networks obtained with or without sequencing bias normalization, but normalized for genomic distance, should behave almost identically. Then, as mentioned in subsection \ref{par:sequencingbias}, sequencing bias normalization methods assume the bins to be homogeneous in regards of the feature they are normalizing for. This is quite the approximation, considering that even at a 10 kb resolution, which is on the lower end of viable resolutions, the bins are still quite big and heterogeneous, to the point where it is arguable whether the entire normalization actually makes sense or not. All of this, coupled with the fact that it is unclear which bias normalization algorithms are actually reliable and the fact that integer counts have substantially better properties for computational purposes, led to the choice of not including sequencing bias normalization in the standard package pipeline. Nevertheless, 
HiCONA is fully capable of working with sequencing bias normalized counts, though in a less efficient manner considering it was optimized for non-normalized ones.

The way HiCONA normalizes for genomic distance was shown to be reliable and consistent, being unstable only at extremely high genomic distances. At genomic distances that high, the actual biological significance of those contacts starts to become debatable, with them being random ligation products or PCR artifacts becoming the more likely option. Losing a few pixels at these genomic distances does not therefore seem like a major issue. In regards of normalization factors computation, it should be noted that, regardless of the usage of \textit{cooltools} or HiCONA, one summary statistic function is computed for each individual chromosome. \textit{Cooltools} does, in fact, go even beyond this, allowing to remove telomeres and centromere, then compute one normalization curve for each arm of the chromosome. The use of one curve per chromomes is to avoid relying on the assumption that random contact probability decays at the same rate in all chromosomes. One might argue that considering all pixels jointly would yield a more robust and less noisy function; while that would indeed be the case, each chromosome has enough pixels to obtain a robust result on its own (aside from chromosome Y due to its significantly smaller size and probably other biological motivations).

Pixel filtering is an interesting topic. The step was introduced to both remove pixels which are definitely to be removed for sparsification to work properly (self-looping and inter-chromosomal ones), as well as to reduce a bit noise and dimensionality prior to sparsification. This last aspect was especially relevant when the tool did not work in chunks and thus table size was an issue. Though raw counts threshold is probably not needed anymore and was, in fact, not included in the analyses presented, different combinations of genomic distance threshold and normalized counts quantile threshold were tested. In terms of filtering itself, the results seem middling; any filtering threshold which provides a significant reduction in pixel number also causes a massive reduction in genomic distance which does not seem to be justifiable prior to network sparsification, making very loose thresholds the only viable option. The need for a filtering step becomes even more debatable when considering that the combination of filtering parameters used does not appear to significantly affect sparsification, if at all. Yet, the choice of filtering parameters affects the consistency on replicates quite a lot, with fold changes which are higher and statistically significant. Further testing on parameter combinations could probably lead to even better consistency.

Regarding network sparsification, a standardized procedure to decide an alpha cutoff was proposed and used throughout all the analyses, removing the need to define arbitrary values, which becomes especially complex when considering that differen thresholds are required for different chromosomes. As mentioned, being sparsification scores actual p-values, the usual p-value cutoff of 0.05 was actually tested as a sparsification score threshold. This value proved far too restrictive with any combination of filtering parameters, to the point where just a couple hundreds of pixels are retained, at most, for each file. This value is therefore not appropriate for this application, and in fact, even the original paper which introduced the algorithm proposed the use of looser thresholds. It must be remarked once more that the objective of this procedure is not to determine the definitely significant pixels themselves, but rather sparsify the network by removing the least significant ones to achieve a network of a manageable size (while still reasonably connected). Both minimal alphas, for a more permissive network, as well as maximal alphas, for a more theoretically robust one, were tested; the choice does not change sparsification results in an appreciable manner, while minimal alphas seem to marginally outperform maximal ones in regards of consistency. For this reason, minimal alphas will be the standard option for the package going forward. The aspect which could be considered the big criticality of the sparsification algorithm is how much the median genomic distance is reduced. While it is fair to assume that there are more significant interactions at closer ranges, the distance reduction feels quite heavy. This is not unexpected, considering that the algorithm does not keep genomic distance into account; one option could be to try and include the distance information in the algorithm, though it seems rather complex and demanding.


\section{Computational performance}

The current implementation of the sparsification algorithm is already quite fast, though many aspects could be tweaked for an even greater speed-up. Regarding integral computation, for instance, the cache of integrals could persist among chunks; this is currently not done since there is a fairly big amount of integrals that are still used only once throughout the entire chromosome sparsification, and keeping all of them in cache needlessly would increase its size drastically. Another option would be to precompute the most common integrals and store them in a tabular file, ready to be loaded as cache when needed. This would lead to a fairly big overhead during the first usage of the package, but could speed-up drastically successive runs. Another aspect which could be optimized are operations on \textit{pandas} dataframes; though vectorized operations are fast, they could be improved by relying on faster structures, such as \textit{polars} dataframes, which work similarly but they are implemented in Rust, making them substantially faster. Dataframe operations could also be sped up using \textit{swifter}, which allows to split a dataframe in chunks and work parallely on them. On this topic, it should be mentioned that HiCONA, as of now, does not perform any form of parallelization explicitly, though some operations are implicitly parallelized by \textit{cooler} using \textit{dask}. A pretty substantial speed-up could be obtained by processing the chromosomes in parallel, considering the fact that they are independent from each other. That being said, the library \textit{h5py}, used to work with \texttt{.hdf5} and on which \textit{cooler} is based, is implemented with several file locks to prevent corrupting the files while working on them, making parallel writing rather difficult. Overall, considering that the preprocessing times are already quite fast, speeding up these aspects is not currently marked as a top priority.

In terms of RAM usage, the preprocessing was shown to be very efficient, requiring very low amounts of memory. It might be debated that chunk size could be increased, to reduce the number of I/O operations therefore speeding up the procedure while still keeping a relatively low memory footprint. Surpisingly, this is not the case; by increasing chunk size the time per million of pixels to process actually increases. Though no definitive reason was found, this is most likely due to a poor scaling of the \texttt{merge} function of \textit{pandas}, which is used to associate the computed integral values back to the pixels. In any case, a better choice would be to keep the amount of memory of the individual process low, then run multiple of them in parallel on different chromosomes as discussed in the previous paragraph. 

\section{Biological validation}

In the results, biological validation through replicate consistency was analyzed. Overall the pipeline does not seem to have the highest Jaccard indexes in terms of absolute value, though the sparsification step is very good at retaining the correlation present in the filtered pixels, especially given the right filtering parameters. When considering that the correlation of the filtered files is already very low, it does appear that the issue with low correlation might lie in the biological experiments themselves rather than in the processing pipeline. This would be in line with the fact the matrix obtained through a single Hi-C experiment is very sparse, therefore requiring multiple experiment to be merged to obtain a denser matrix. The ideal pipeline would be able to increase the correlation among replicates, since this would mean that similar information could be obtained by sequencing less, thus reducing experimetal costs. HiCONA does not seem to be at that stage yet.

Another form of biological validation which was performed, but not shown in the results since not strictly tied to the actual implementation, regards functional annotation enrichment. For each bin, it is possible to define which functional element (promoter, enhancer, heterochromatin and others) is the most enriched with respect to the abundance of these functional elements in the genome. Then, it is possible to define the fraction of interactions which occur among any pair of functional elements. By comparing these fractions among different processing steps, such as between total pixels and sparsified pixels, it is possible to define which type of interactions are enriched by the procedure. Ideally, one would like for biologically relevant annotations, such as promoters and enhancer, to be enriched by the step. This analysis was conducted on IMR90 and GM12878 at 5 kb resolution and yielded promising results, with a substantial enrichment of promoter, enhacer ad zinc finger binding annotations. 

%%%%%%%%%%%%% ABOUT THE PACKAGE %%%%%%%%%%%%%%%%%%%%%%%

\section{Network analysis algorithms}

In HiCONA, a \texttt{HiconaGraph} object is already implemented. Since it inherits from the main \textit{graph-tool} class, it is able to already perform quite a vast set of operations, such as computing network statistics and centrality measure, performing clustering and complex plotting (but the list is much longer). To these functionalities, an \textit{ad hoc} version of node label permutations was added. The algorithm does work properly on mock networks, it does not seem to work on real Hi-C derived ones. Though this could simply be that there are no differences in network properties of nodes annotated with different biological function, this could also be due to how the node annotation is performed. Currently, bins are annotated by intersecting them with a bed-like file and assigning the label to all of them with any amount of overlap. It must be considered though that bins of 5 kb in size are quite big and thus many bins end up being annotated with a lot of labels. This huge overlap could be what . 


% Moreover, contrast subgraphs \cite{contrast2020, contrast2023}, a more recent algorithm with promising biological perspectives, has also been implemented.

% In the animal-community studies field, it has been discussed for a while how node-labels permutation does not actually perform as well as initially though given that most permutations do not satisfy the joint probabilities smth smth dont remember exactly [FIX] \cite{nullmodel2017}; fairly recently, a theoretical proof that node-label permutation perform exactly like non-network-based parametric methods has been published [ADD REFERENCE]. That being said, the network approach INSERT REASON WHY THE NETWORK APPROACH IS STILL GOOD, LIKE INTERPRETABILITY AND SUCH.

\section{Package development}

Regarding package implementation, HiCONA is 